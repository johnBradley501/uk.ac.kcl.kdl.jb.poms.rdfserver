This is the eclipse project that is used to maintain the PoMS RDF server found at http://www.poms.ac.uk/rdf

It was created by John Bradley (DDH/KCL) in the first half of 2019, and is used to provide the REF-oriented services
for PoMS.

There are two jobs that must be done to set up the server:
(I) Setting up the repository for the RDF triple data. This involves establishing a place on the
server's machine that is suitable (I suggest using /usr/local/etc/rdf4j as a suitable folder), and
after that has been settled, using rdf4j tools, and the tools that I have created to create the RDF
triples and then to load them into this folder.  See section (II), below.

(II) Installing a tomcat server, and install the RDF server app within it.  One of the issues associated
with this is setting up the URI mapping so that requests from browsers are directed to the server.  See
section (I.a), below.

Section III, below, describes the changes that have been made to the Java code to make the server
work as needed.




(I) Preparing data for serving as RDF
======================================
The RDF triple data for the PoMS RDF server comes primarily from PoMS's MySQL database. It is supplemented by
the RDF statements in the PoMS ontology file ontoloy/poms.owl, which allows the RDF server to use some of
its logical "reasoner" facilities to further enrich the data.

As we shall see, almost all the RDF triples are automatically generated by the d2rq toolkit, but some of the
data in the Place object -- data which supports the GeoSPARQL querying mechanisms -- cannot be exported
correctly with d2rq, so a small Python script has been created to generated triples for this data.

Thus, there are four steps to preparing data for the PoMS RDF server:
(1.a) using d2rq to generate the bulk of the RDF data
(1.b) using a small Java program to clean up the generated data: removing RDF statements that refer to "empty" values.
(2) using the python script misc/getGeom.py to generate the GeoSPARQL data
(3) using the rdf4j console to load the data created from 1.a, 1.a, 2, and the poms.owl ontology triples
into the triple store at /usr/local/etc/rdf4j

I.1) Creating the RDF Triples
-----------------------------
There is some information about how this task was carried out for DPRR in the public description of the DPRR RDF server at
http://romanrepublic.ac.uk/rdf/doc. See tab "Building the Server". The process for PoMS is very similar, but not quite the
same!!

The d2rq toolkit (http://d2rq.org/) is used as the mapping engine from the DB to RDF.  It requires
a "mapping file" (see documentation at http://d2rq.org/d2rq-language) to guide the translation, and
this file has been created so that d2rq tools can be used (see 1.a) to generate a set of raw
RDF triples.  However, d2rq, by itself, does not create the actual version of RDF data
that is to be loaded.  There is, then, a 2nd step (1.b) involving a small Java program to complete the
preparation.

The mapping file that has been developed for the PoMS data can be found in the project folder: misc/poms-map.ttl

N.B: The d2rq process assumes that the MySQL database structure is THE SAME as that used when the
process was defined. If the structure is changed, the d2rq "mapping file" (mentioned above) needs to be changed
to reflect the changes in the DB's structure before the triples can be satisfactorily exported.

I.1.a) Using d2rq to create base RDF triples
--------------------------------------------
Use d2rq's dump-rdf tool (http://d2rq.org/dump-rdf) to export data into RDF.  I use the command line

dump-rdf -f TURTLE -b http://www.poms.ac.uk/rdf/entity/ -o poms_dump.ttl poms-map.ttl

for this purpose. Note the following:
(i) -f: creates the RDF dump of the data in RDF's TURTLE format.
(ii) -b: the given base URI causes all data entities to be given the required URI prefix in the exported RDF.
(iii) -o: the generated RDF triples are put into the file poms_dump.ttl.
(iv) the conversion is controlled by the mapping file poms-map.ttl, which is available from
this project's "misc" subfolder.

dump-rdf connects directly to PoMS's MySQL database for this operation.  Hence, be sure to put appropriate
credentials into the PoMS-map.ttl file before running it. You'll find the items you need to change near the
top of the file.

Finally, on my machine this process runs for a long time (more than an hour).  Expect this.

I.1.b) Creating final RDF triples with Python file
--------------------------------------------------
A small Java program takes the RDF triple data created by dump-rdf and "cleans it up" by eliminating
the many "empty" triples that dump-rdf creates.

The Java program is in package "uk.ac.kcl.kdl.jb.rdf.util", and is called script is called RemoveEmpties.java. If you are
using Eclipse, it is easiest to simply run it there.  As you can see from the code, it takes two parameters, the first is the
path and name of the input file (created by step 1.a: <your path>/poms_dump.ttl, and the 2nd is the name of the file it is
to generate. I suggest <your path>/poms_clean.ttl.

I use these parameters to run this program:
d:/research/PoMS-LOD/d2rq-0.8.1/poms_dump.ttl d:/research/PoMS-LOD/d2rq-0.8.1/poms_clean.ttl

but the path you will need will doubtless be different.

This program can be run as a java program outside of an Eclipse environment too, but be aware that it draws on rdf4j classes
to provide reading and writing RDF services.

In the discussion below I am assuming that the output file is called poms_clean.ttl.

I.2) Creating the Geographic RDF statements
-------------------------------------------
There is geographic point data in the MySQL database associated with the Place table, and it can be used in PoMS's RDF server.
Unfortunately, I was unable to get d2rq's dump-rdf program to extract the data and write it in the correct format.  Thus, I have
written a small python script which connects to the MySQL database, reads the data there, and writes it out as RDF in the
appropriate format into a file called "poms_geom.ttl"

The Python script is misc/getGeom.py. It has hardcoded into it the connection data for the database, and the path and name for
the file it is to generate.  I suggest that you edit the file and change this to match what you need.

This script is written for Python 2.7 and makes use of the MySQLdb module to support connection to the MySQL database.
If you are using Python 3.0 or use another module to support the database connection you will likely find that you need to
tweek it a little to make it work.

I.3) Loading RDF into rdf4j repository
--------------------------------------
Having created the triples, and stored them in poms_clean.ttl (steps 1.a and 1.b above), and poms_geom.ttl (step 2 above),
one uses rdf4j's console program to create a suitable RDF repository.  The console program is described at
http://docs.rdf4j.org/server-workbench-console/. I have used rdf4j version 2.5.1, but the current 2.5.2 is likely to
work as well.

Be aware the PoMS's rdf4j repository contains a reasoner that allows the rules in PoMS's ontology to enrich the data
display and SPARQL querying. To enable the reasoner, the PoMS repository is set up as a "native-rdfs-dt" type (see section 2.1.3
of the console documentation mentioned above). Thus, BOTH the PoMS data (created above), and the PoMS ontology needs to be loaded.
You can find the ontology file in the WebContent as file "poms.owl" in the ontology folder.

Because of a quirk in the console, ontology/poms.owl must have a "ttl" file suffix.  Thus, it is necessary to copy the file
from there and give the copy a new name, say "poms_ont.ttl".

The rdf4j console needs extra heap memory in order to successfully load the over 3 million triples that represent the PoMS data.
To achieve this I have modified my version of the command.bat script provided by rdf4j to run the command program specifying
mote heap memory: "-Xmx4096m". You will need to make a similar modification in the command.sh file by setting the JAVA_OPT variable
found there: JAVA_OPT=-Xmx4096m

One interacts with console through a serious of commands.  Here are the ones I use (on my Windows machine)
to load the new data:

connect d:/data/rdf4j (on a Linux machine, the path to the data repository will be something like /usr/local/etc/rdf4j)
show repositories
drop poms    (if a previous version currently exists)
create native-rdfs-dt (followed by a sequence of prompts for parameters -- see below)
open poms
load D:/research/poms-lod/d2rq-0.8.1/poms_clean.ttl
load d:/research/poms-lod/d2rq-0.8.1/poms_geom.ttl
load d:/research/poms-lod/Ontology/poms_ont.ttl 
exit

Comments:
-- First, I connect to the place on my machine where rdf4j repositories are stored.  I have suggested on the actual
   linux based server this should be /usr/local/etc/rdf4j, in which case the command would be "connect /usr/local/etc/rdf4j".
-- The "show repositories" command is not necessary, but will ensure that you are pointing at the right place.  console
   should respond with at least two repositories -- any existing "poms" repository and the "SYSTEM" repository.
-- delete the old version of the poms repository
-- create the new poms repository using the native-rdf2-dt format.  You will receive a sequence of prompts for information. Values to give are:
   * Repository ID: poms
   * Repository title: PoMS data as RDF
   * Triple Indices: spoc, posc, opsc
   * for all others accept the proposed default values
    
-- If all is OK so far, use the "open poms" command to point the console at the  newly created poms repository.
-- You are now ready to load the RDF files created earlier.  You will, of course, in each case need to specify
   the correct path for your machine to the directory that is holding the three pieces of data.
-- I suggest this order
      -- First, loading the bulk of the data created by d2rq and RemoveEmpties.java.  This takes a very long
         time;  on my machine almost an hour.
      -- Second, loading the geographic point data that had been created by the getGeom.py script. Although there are
         not very many triples in it, this also takes some time: more than 20 minutes on my machine.
      -- Third, load the ontology file (renamed to poms_ont.ttl) This is the smallest file of all, but still takes
         about 40 minutes to load on my machine!!



(II) Setting up the Server for Public Use
========================================
This server is built upon the workbench code from rdf4j (http://rdf4j.org) version 2.5.1.
-- it operates as a Java-based web application, and runs under Tomcat.
-- rdf4j requires at least a Java-based web server that supports at least Java Servlet API 2.5 and 
   Java Server Pages (JSP) 2.0, or newer.  I have developed and tested it with Tomcat 8.0 and 8.5. There
   is good reason to expect that it will also run under the latest version of Tomcat: version 9.
-- rdf4j requires at least Java version 8 (see https://rdf4j.eclipse.org/download/), I have also tested
   it with Java 11.

The PoMS RDF app can be packaged as a .war file, and then set up using Tomcat's WAR file support.  A
suitable WAR file can be readily exported from this Eclipse project.  I recommend it should be 
called "rdf.war", and put in the project's folder.

In the default version of this app, RDF data is fetched from /usr/local/etc/rdf4j.  Within this folder
is the rdfj4 repository folder called "PoMS", which is used as the source for the RDF data to be served.
Both the folder for RDF repositories and the particular folder can be changed by editing file
/WEB-INF/server.config.

See information below about how to get data from PoMS's MySQL database into suitable RDF format.

(II.a) URI mapping requirements
==============================
It is one of the four basic Linked Data requirements that a user needs to be able to enter a URI for an entity, 
and receive useful data back.  URIs for entities in PoMS's RDF data all begin "http://www.poms.ac.uk/rdf/entity",
with "entity" being the path component recognised by PoMS RDF Server as a request for entity data.

"http://www.poms.ac.uk/rdf/ontology" is described as a request for the PoMS ontology, and
"http://www.poms.ac.uk/rdf/endpoint" is a request to access the data-oriented SPARQL endpoint.
"http://www.poms.ac.uk/rdf/doc" is a request to access the documentation about the PoMS RDF Server.

All of rdf4j's native functionality that is supported within PoMS RDF server should begin with

"http://www.poms.ac.uk/rdf/repositories/poms"

Thus, all URLs that this tomcat application serves begin "http://www.poms.ac.uk/rdf" 
and the larger server environment needs to be set up so that "http://www.poms.ac.uk/rdf" 
is passed to this PoMS web application.

N.B.: It is important that all mapping from this URL prefix to whatever is needed to get the request
to the web app should be done "behind the scenes", without changing the URL the user sees!!

(II.b) WEB-INF/server.config
----------------------------
Servlet code in the app get some of the data which they need to operate from two files in WEB-INF:
  -- many options for the rdf4j native code come from the web.xml file.
  -- other options, used by both the native code, and the code that I have developed come from
     file server.config. This file was not needed in the original materials for the rdf4j workbench.
     
The server.config file is structured using the conventions of Java's java.util.Properties representation:
usually lines in a text file, each line begins with an property name, followed by a "=" or ":", followed
by the value to be assigned.  The options are:

server: the place where the triple store repository (created in section I, above) is located
path: the folder within the server where the specific repository is located: here "/poms"
copyright: wording for a copyright text to be used in data generated by this application
uriPrefix: the prefix for all URIs that refer to PoMS's data, here: http://www.poms.ac.uk/rdf/entity/
ontologyPath: the path to and filename for the ontology file, here: /ontology/poms.owl

     
         
(III) Differences between rdf4j workbench and the PoMS RDF server
=================================================================
The PoMS RDF server is built upon the rdf4j workbench code, but has several important differences:

(a) the full rdf4j workbench supports not only the reading of RDF data, but also its changing.  Hence,
all the code that supports changing of the RDF data has been removed from the PoMS workbench.

(b) normally the workbench fetches all its data from a backing rdf4j RDF server.  The workbench is then a
user-friendly front end to operations that can be done against this backend server.  In the PoMS server
data is fetched from a representation of the RDF data directly in the file system (at /usr/local/etc/rdf4j)
rather than through an intermediate RDF server. This is to prevent the world from being able to get at
the RDF data through this intermediate server, where functions such as data changing, are provided. You
can see the PoMS RDF server being pointed to the file system containing the PoMS RDF data rather than
to an intermediate backing rdf4j server in /WEB-INF/server.config where the server is specified using a
file URL: "file:/usr/local/etc/rdf4j"

(c) some functionality has been added to the PoMS server by John Bradley.  They are found in JAVA package
uk.ac.kcl.ddh.jb.rdf.server and uk.ac.kcl.ddh.jb.rdf.server.servlet, and are referenced in the
WEB-INF/web.xml file:

(c.1) an entity servlet (EntityServlet) allows users to pass URIs for PoMS data entities (e.g.
http://www.poms.ac.uk/rdf/entity/Person/1, etc) and receive
back useful information either in the form of an HTML page, or a collection of RDF statements that are
related to the specified entity. This is actually achieved by either making use of rdf4j's ExploreServlet
for the HTML-oriented interface, or by invoking the added "subservlet" RdfGenServlet which delivers raw RDF data.
(c.2) a SPARQL Endpoint servlet (EndpointServlet) provides data-oriented SPARQL endpoint services for users.
(c.3) a servlet that aims to serve the PoMS OWL Ontology is provided in OntologyServlet. See (d), below.

(d) The PoMS Ontology is made available through the web content folder "/ontology".

(d.1) The ontology itself that is in there and called poms.owl is actually served to users through the rather 
trivial OntologyServlet. It knows where the ontology file is by looking up its location in property 
"ontologyPath" in file WEB-INF/server.congif, which is initialised to "/ontology/poms.owl"
(d.2) The documentation about the ontology, created by OWLDoc, is served from the folder /ontology/doc

(e) Perhaps the most significant change to rdf4j code, other than the removing of functions mentioned 
in point (a) above, is the addition of a handler for the rdf4j RepositoryManager used here.  The code
for this is in uk.ac.kcl.ddh.jb.rdf.server.SharedRepositoryHandler.  It holds the RepositoryManager
that gives program access to the rdf data (normally in /usr/local/etc/rdf/).  See the description of
this functionality in the next section of this file. To make use of SharedRepositoryHandler required
minor changes in rdf4j-provided code in classes in org.eclipse.rdf4j.workbench.proxy: WorkbenchGateway
and WorkbenchServlet, org.eclipse.rdf4j.workbench.util.BasicServletConfig, and
org.eclipse.rdf4j.workbench.base.TransformationServlet.  See more details below.

(f) folder WebContent/doc has been added containing html and associated files and served by the web app.
It provides user documentation about the PoMS RDF server.

Removal of query saving support in workbench code: rdf4j's QueryServlet
-----------------------------------------------------------------------
The rdf4j workbench provides support for the saving and sharing of SPARQL queries between users.  Although
this is a lovely idea in many ways, it seemed to be to be a possible longterm maintenance headache in the PoMS server.
Thus, code to support this was removed.  It was easy to the remove the code in the web content files that presented this
feature to the user on his/her browser.  However, I chose to also remove support from the backing Java code in the rdf4j
workbench.  The change was made in org.eclipse.rdf4j.workbench.commands.QueryServlet by commenting out a private function
altogether (lines 280-317), and then removing the code that referenced it lines 242-44.  This code could have perhaps been
left as rdf4j had delivered it since the web-content changes made its functionality invisible to the PoMS RDF server user.

uk.ac.kcl.ddh.jb.rdf.server.SharedRepositoryHandler
---------------------------------------------------
The SharedRepositoryHandler was added to deal with two problems that arose from the rdf4j code with
regard to the RepositoryHandler.  In the original rdf4j code the location of the source for RDF data
and the particular repository for it was made available to the workbench code as Servlet InitParameter
data for rdf4j's servlet WorkbenchGateway in web.xml:

-------------------------------------------
		<init-param>
			<param-name>default-server</param-name>
			<!--  param-value>/rdf4j-server</param-value -->
			<param-value>file:/usr/local/etc/rdf/</param-value>
		</init-param>

[...]
		<init-param>
			<param-name>default-path</param-name>
			<!--  param-value>/NONE/repositories</param-value -->
			<param-value>/PoMS</param-value>
		</init-param>

-------------------------------------------
This worked well for the "native" rdf4j workbench since all the rdf4j functionality came through a single servlet
called "workbench": org.eclipse.rdf4j.workbench.proxy.WorkbenchGateway

However, in the PoMS RDF server it was most expedient to add two other servlets for entity display
and for the basic SPARQL endpoint service (EndpointServlet and EntityServlet) in addition to
WorkbenchGateway, and they didn't have access to the init parameters for WorkbenchGateway.

In addition, and even more serious, the rdf4j RepositoryHandler (LocalRepositoryHandler) that
provided program access to the RDF repository only allowed one rdf4j Repository class to be instantiated.
(this was not true if the intermediate rdf4j server was used instead -- but see item (b), above)
As a result, this one instance had to be shared between the rdf4j's surviving workbench code, and the servlets
developed specifically for the PoMS RDF server: EndpointServlet and EntityServlet.

Thus, the SharedRepositoryHandler code was developed that took its repository information from
/WEB-INF/server.config rather than from Servlet InitParameters.  It created and held an rdf4j
RepositoryHandler instance that could be accessed by both rdf4j and PoMS RDF servlets.

The effect to rdf4j was that the default-server and default-path parameters have been removed from
the /WEB-INF/web.xml file for PoMS RDF, and the rdf4j code changed in a few places so that the data
for the location of the RDF repository is now taken from the SharedRepositoryHandler rather than
from Servlet InitParameters.

Changes to rdf4j code
---------------------
The following changes have been made to rdf4j code to make use of SharedRepositoryHandler:
-- org.eclipse.rdf4j.workbench.proxy.WorkbenchGateway: methods getDefaultServerPath() and init()
-- org.eclipse.rdf4j.workbench.proxy.WorkbenchServlet: method init()
-- org.eclipse.rdf4j.workbench.util.BasicServletConfig: constructor BasicServletConfig(String name, ServletContext context)
-- org.eclipse.rdf4j.workbench.base.TransformationServlet: method init()

         